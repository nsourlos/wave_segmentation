{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb68e12",
   "metadata": {},
   "source": [
    "# LangSAM to predict wave contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc00a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from lang_sam import LangSAM\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import psutil\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c986dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify paths\n",
    "video_path = os.path.join(\"workspace\", \"original_video.mp4\") #This will be split into frames\n",
    "frames_path = os.path.join(\"workspace\", \"frames_wave\") #This is where the original video frames will be saved\n",
    "save_path = os.path.join(\"workspace\", \"results_wave\") #This is where the output frames with the mask will be saved\n",
    "\n",
    "combined_video_path = os.path.join(save_path, \"combined_output_video.mp4\")\n",
    "mask_video=os.path.join(save_path,'output_mask_video.mp4')\n",
    "final_video=os.path.join(save_path,'output_video.mp4')\n",
    "\n",
    "#If paths do not exist, create them\n",
    "if not os.path.exists(frames_path):\n",
    "    os.makedirs(frames_path)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a149fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\" to /root/.cache/torch/hub/checkpoints/sam2.1_hiera_large.pt\n",
      "100%|██████████| 856M/856M [00:19<00:00, 46.9MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1546eef059d34bce830d3e38c8a47120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/457 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e2ec62951040cf822b72d1bef24889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaf22902f2d4cd9afb24fc1032aaf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbeacd8b3a447bc8235f65425ea1f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb03e36681427c97ef9239879f1608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28ce8f78d5d4605b278c301f8d3db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7165ecfb23f4b4dab9ea3b3bf75f230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/933M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LangSAM(sam_type=\"sam2.1_hiera_large\") #Default is \"sam2.1_hiera_small\"\n",
    "text_prompt = \"wave.\" #This is what we want to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e772061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames from video...\n",
      "Total frames: 602\n",
      "Processed 100 frames...\n",
      "Processed 200 frames...\n",
      "Processed 300 frames...\n",
      "Processed 400 frames...\n",
      "Processed 500 frames...\n",
      "Processed 600 frames...\n",
      "Extraction complete. 602 frames saved to workspace/frames_wave\n"
     ]
    }
   ],
   "source": [
    "def extract_frames(video_path, output_path=frames_path):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Initialize frame counter\n",
    "    count = 0\n",
    "    \n",
    "    print(f\"Extracting frames from video...\")\n",
    "    print(f\"Total frames: {frame_count}\")\n",
    "    \n",
    "    # Read and save frames\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Save frame as JPEG file\n",
    "        frame_path = os.path.join(output_path, f'frame_{count:04d}.jpg')\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} frames...\")\n",
    "    \n",
    "    # Release video capture object\n",
    "    video.release()\n",
    "    \n",
    "    print(f\"Extraction complete. {count} frames saved to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186bf27",
   "metadata": {},
   "source": [
    "Plot the mask on top of original frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.array([*np.random.random(3), 1.0])\n",
    "    else:\n",
    "        color = np.array([0/255, 255/255, 0/255, 1.0])  # Changed to green (RGB: 0,255,0) from blue 30/255, 144/255, 255/255\n",
    "\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    \n",
    "    # Create empty image for contours only\n",
    "    mask_image = np.zeros((h, w, 4))\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    # Smooth contours\n",
    "    contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "    # Draw contours with thickness=1 for single pixel width\n",
    "    cv2.drawContours(mask_image, contours, -1, color, thickness=1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, save_path=None, mask_path=None, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    \n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        # Original image with mask\n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        # Create figure with exact pixel dimensions\n",
    "        fig1 = plt.figure(figsize=(original_width/100, original_height/100), dpi=100)\n",
    "        ax1 = plt.Axes(fig1, [0., 0., 1., 1.])\n",
    "        fig1.add_axes(ax1)\n",
    "        \n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=100, bbox_inches=None, pad_inches=0)\n",
    "\n",
    "        # Save mask only\n",
    "        fig2 = plt.figure(figsize=(original_width/100, original_height/100), dpi=100)\n",
    "        ax2 = plt.Axes(fig2, [0., 0., 1., 1.])\n",
    "        fig2.add_axes(ax2)\n",
    "        \n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        \n",
    "        if mask_path:\n",
    "            plt.savefig(mask_path, dpi=100, bbox_inches=None, pad_inches=0)\n",
    "\n",
    "        # Force cleanup\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b307fd",
   "metadata": {},
   "source": [
    "Create frames of mask and of final segmented object (±1sec/frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdeffd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/602 [00:00<?, ?it/s]/workspace/myenv/lib/python3.11/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
      "Processing frames: 100%|██████████| 602/602 [09:27<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# import tracemalloc\n",
    "# tracemalloc.start()\n",
    "\n",
    "files = sorted(os.listdir(frames_path))\n",
    "for ind, path_img in tqdm(enumerate(files), desc=\"Processing frames\", total=len(files)):\n",
    "    # if ind > 260: #Activate this if we run out of memory and have to restart from specific frame\n",
    "\n",
    "        # Track memory usage before processing\n",
    "        process = psutil.Process()\n",
    "        # print(f\"\\nMemory usage before frame {ind}:\")\n",
    "        # print(f\"RAM used: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        image_path = frames_path+'/'+path_img\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        try:\n",
    "            results = model.predict([image_pil], [text_prompt])\n",
    "            # print(results)\n",
    "            show_masks(image_pil,\n",
    "                    results[0]['masks'],\n",
    "                    results[0]['scores'], \n",
    "                    save_path=save_path+'/output_'+path_img,\n",
    "                    mask_path=save_path+'/output_mask_'+path_img,\n",
    "                    point_coords=results[0]['boxes'][0],\n",
    "                    input_labels=results[0]['labels'],\n",
    "                    borders=True)\n",
    "            \n",
    "            # Clean up variables explicitly\n",
    "            del results\n",
    "            image_pil.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            if results[0]['masks'] is None:\n",
    "                print(\"No masks found\")\n",
    "            else:\n",
    "                print(f\"Error: {e}\")\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Check memory usage every 20 frames\n",
    "        if ind > 0 and ind % 10 == 0:\n",
    "            ram_used = process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "            if ram_used > 10000:  # If using more than 10GB RAM\n",
    "                print(f\"\\nHigh RAM usage detected ({ram_used:.0f}MB) at frame {ind}\")\n",
    "\n",
    "                # # Memory snapshot for analysis\n",
    "                # snapshot = tracemalloc.take_snapshot()\n",
    "                # top_stats = snapshot.statistics(\"lineno\")\n",
    "                # print(\"\\n[Top Memory Consumers]\")\n",
    "                # for stat in top_stats[:5]:\n",
    "                #     print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ecd20",
   "metadata": {},
   "source": [
    "Save frames as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1d96c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as workspace/results_wave/output_video.mp4\n",
      "Mask video saved as workspace/results_wave/output_mask_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Get list of image files and sort them\n",
    "image_files = sorted([f for f in os.listdir(save_path) if f.endswith(('.png', '.jpg', '.jpeg')) and 'output' in f and 'output_output' not in f and 'output_mask' not in f])\n",
    "mask_files = sorted([f for f in os.listdir(save_path) if f.endswith(('.png', '.jpg', '.jpeg')) and 'output_mask' in f])\n",
    "\n",
    "if len(image_files) > 0:\n",
    "    # Read first image to get dimensions\n",
    "    first_image = cv2.imread(os.path.join(save_path, image_files[0]))\n",
    "    height, width, layers = first_image.shape\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(final_video, fourcc, 30.0, (width, height)) #30 is the frame rate\n",
    "\n",
    "    # Add each image to video\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(save_path, image_file)\n",
    "        frame = cv2.imread(image_path)\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release video writer\n",
    "    out.release()\n",
    "    print(f\"Video saved as {final_video}\")\n",
    "else:\n",
    "    print(\"No images found in the directory\")\n",
    "\n",
    "#Save mask\n",
    "if len(mask_files) > 0:\n",
    "    # Read first image to get dimensions\n",
    "    first_image = cv2.imread(os.path.join(save_path, mask_files[0]))\n",
    "    height, width, layers = first_image.shape\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(mask_video, fourcc, 30.0, (width, height)) #30 is the frame rate\n",
    "\n",
    "    # Add each image to video\n",
    "    for image_file in mask_files:\n",
    "        image_path = os.path.join(save_path, image_file)\n",
    "        frame = cv2.imread(image_path)\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release video writer\n",
    "    out.release()\n",
    "    print(f\"Mask video saved as {mask_video}\")\n",
    "else:\n",
    "    print(\"No mask images found in the directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890255be",
   "metadata": {},
   "source": [
    "Create a figure with the original video, the mask video and the overlay video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b853821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1 - Width: 1024, Height: 1024, FPS: 30\n",
      "Video 2 - Width: 1024, Height: 1024, FPS: 30\n",
      "Video 3 - Width: 1024, Height: 1024, FPS: 30\n",
      "Combined video saved as workspace/results_wave/combined_output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Open the video files\n",
    "cap1 = cv2.VideoCapture(video_path)\n",
    "cap2 = cv2.VideoCapture(mask_video)\n",
    "cap3 = cv2.VideoCapture(final_video)\n",
    "\n",
    "# Get video properties from the first video\n",
    "fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create a VideoWriter object to save the combined video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "out = cv2.VideoWriter(combined_video_path, fourcc, fps, (width * 3, height))\n",
    "\n",
    "while True:\n",
    "    # Read frames from each video\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "    ret3, frame3 = cap3.read()\n",
    "\n",
    "    # Break the loop if any video ends\n",
    "    if not ret1 or not ret2 or not ret3:\n",
    "        break\n",
    "\n",
    "    # Resize frames to the same height (optional, but recommended)\n",
    "    frame1 = cv2.resize(frame1, (width, height))\n",
    "    frame2 = cv2.resize(frame2, (width, height))\n",
    "    frame3 = cv2.resize(frame3, (width, height))\n",
    "\n",
    "    # Combine frames side by side\n",
    "    combined_frame = np.hstack((frame1, frame2, frame3))\n",
    "\n",
    "    # Write the combined frame to the output video\n",
    "    out.write(combined_frame)\n",
    "\n",
    "print(f\"Video 1 - Width: {int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))}, Height: {int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))}, FPS: {int(cap1.get(cv2.CAP_PROP_FPS))}\")\n",
    "print(f\"Video 2 - Width: {int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))}, Height: {int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))}, FPS: {int(cap2.get(cv2.CAP_PROP_FPS))}\")\n",
    "print(f\"Video 3 - Width: {int(cap3.get(cv2.CAP_PROP_FRAME_WIDTH))}, Height: {int(cap3.get(cv2.CAP_PROP_FRAME_HEIGHT))}, FPS: {int(cap3.get(cv2.CAP_PROP_FPS))}\")\n",
    "\n",
    "# Release everything when done\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cap3.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Combined video saved as {combined_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db973a3",
   "metadata": {},
   "source": [
    "# SAM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6715015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ai.meta.com/blog/segment-anything-2/?utm_source=twitter&utm_medium=organic_social&utm_content=reel&utm_campaign=sam2\n",
    "#Load image from desktop\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import torch\n",
    "# import cv2\n",
    "# from sam2.build_sam import build_sam2\n",
    "# from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "# from sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a5ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = frames_path+\"/frame_0180.jpg\"#\"/workspace/wave1.png\"#os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"wave1.png\")\n",
    "# image = Image.open(image_path)\n",
    "# image_np = np.array(image.convert(\"RGB\"))\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d00846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_url=\"https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d27885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Taken from https://github.com/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "# # checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "# # model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "# # checkpoint = \"/workspace/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "# # model_cfg = \"/workspace/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "# checkpoint = desktop_path+\"/sam2.1_hiera_large.pt\"\n",
    "# model_cfg = desktop_path+\"/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "# # mask_generator = SAM2AutomaticMaskGenerator(build_sam2(model_cfg, checkpoint, device=\"cuda\"))\n",
    "# sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\", apply_postprocessing=False)\n",
    "# mask_generator=SAM2AutomaticMaskGenerator(\n",
    "#     model=sam2,\n",
    "#     points_per_side=60,\n",
    "#     # points_per_batch=128,\n",
    "#     # pred_iou_thresh=0.7,\n",
    "#     # stability_score_thresh=0.92,\n",
    "#     # stability_score_offset=0.7,\n",
    "#     # crop_n_layers=1,\n",
    "#     # box_nms_thresh=0.7,\n",
    "#     # crop_n_points_downscale_factor=2,\n",
    "#     # min_mask_region_area=25.0,\n",
    "#     # use_m2m=True,\n",
    "# )\n",
    "\n",
    "# masks = mask_generator.generate(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b117b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(3)\n",
    "\n",
    "# def show_anns(anns, borders=True):\n",
    "#     if len(anns) == 0:\n",
    "#         return\n",
    "#     sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_autoscale_on(False)\n",
    "\n",
    "#     img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "#     img[:, :, 3] = 0\n",
    "#     for ann in sorted_anns:\n",
    "#         m = ann['segmentation']\n",
    "#         color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "#         img[m] = color_mask \n",
    "#         if borders:\n",
    "#             contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "#             # Try to smooth contours\n",
    "#             contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "#             cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "#     ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b20b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 20))\n",
    "# plt.imshow(image)\n",
    "# show_anns(masks)\n",
    "# plt.axis('off')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d317d",
   "metadata": {},
   "source": [
    "Image Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ede0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "# # model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "# predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "\n",
    "# input_point = np.array([[300, 150]])\n",
    "# input_label = np.array([1])\n",
    "\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "#     predictor.set_image(image_np)\n",
    "#     masks, scores, logits = predictor.predict(point_coords=input_point,\n",
    "#     point_labels=input_label,\n",
    "#     multimask_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09015a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(3)\n",
    "\n",
    "# def show_mask(mask, ax, random_color=False, borders = True):\n",
    "#     if random_color:\n",
    "#         color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "#     else:\n",
    "#         color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "#     h, w = mask.shape[-2:]\n",
    "#     mask = mask.astype(np.uint8)\n",
    "#     mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "#     if borders:\n",
    "#         contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "#         # Try to smooth contours\n",
    "#         contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "#         mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "#     ax.imshow(mask_image)\n",
    "\n",
    "# def show_points(coords, labels, ax, marker_size=375):\n",
    "#     pos_points = coords[labels==1]\n",
    "#     neg_points = coords[labels==0]\n",
    "#     ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "#     ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "# def show_box(box, ax):\n",
    "#     x0, y0 = box[0], box[1]\n",
    "#     w, h = box[2] - box[0], box[3] - box[1]\n",
    "#     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "# def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "#     for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(image)\n",
    "#         show_mask(mask, plt.gca(), borders=borders)\n",
    "#         if point_coords is not None:\n",
    "#             assert input_labels is not None\n",
    "#             show_points(point_coords, input_labels, plt.gca())\n",
    "#         if box_coords is not None:\n",
    "#             # boxes\n",
    "#             show_box(box_coords, plt.gca())\n",
    "#         if len(scores) > 1:\n",
    "#             plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "877669ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load image from desktop\n",
    "# path='./test_video/'\n",
    "# for path_img in os.listdir(path):\n",
    "#     image_path = path+path_img\n",
    "#     image = Image.open(image_path)\n",
    "#     image_np = np.array(image.convert(\"RGB\"))\n",
    "#     show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e096840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "\n",
    "# masks, scores, _ = predictor.predict(\n",
    "#     point_coords=input_point,\n",
    "#     point_labels=input_label,\n",
    "#     mask_input=mask_input[None, :, :],\n",
    "#     multimask_output=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d138749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
